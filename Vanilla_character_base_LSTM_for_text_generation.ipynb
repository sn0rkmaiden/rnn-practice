{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1y0QUwWpJUNyrXBds3QkpeRyhUgJ0qQ-G",
      "authorship_tag": "ABX9TyPBL2tY2OAP8vRqMjSodtqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sn0rkmaiden/rnn-practice/blob/main/Vanilla_character_base_LSTM_for_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6slPkPAxAp6",
        "outputId": "5795bbd1-448d-4626-ee15-aab005697b57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/235.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hr1HqPHEox6Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Numerically stable version of the logistic sigmoid function\"\"\"\n",
        "    pos_mask = (x >= 0)\n",
        "    neg_mask = (x < 0)\n",
        "    z = np.zeros_like(x)\n",
        "    z[pos_mask] = np.exp(-x[pos_mask])\n",
        "    z[neg_mask] = np.exp(x[neg_mask])\n",
        "    top = np.ones_like(x)\n",
        "    top[neg_mask] = z[neg_mask]\n",
        "    return top / (1 + z)"
      ],
      "metadata": {
        "id": "QgNHVqsoAP6g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SQ6XJhICxA78"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "# file_path=\"/content/drive/MyDrive/shakespeare.txt\"\n",
        "# file_path=\"/content/drive/MyDrive/alice_in_wonderland.txt\"\n",
        "file_path=\"/content/drive/MyDrive/cpp.txt\"\n",
        "\n",
        "data = unidecode.unidecode(open(file_path).read())\n",
        "vocab_size = len(data)\n",
        "print('file_len =', vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJXZfnqOxCz9",
        "outputId": "106c3c73-e8bf-4ef5-c448-eabf98865d74"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 808920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "data_size, vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2GvwN8zxEV2",
        "outputId": "b13f88e6-f84d-4e39-b6da-4a78950b7dd2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(808920, 97)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
      ],
      "metadata": {
        "id": "jbY7Q536NjAf"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_len = 500\n",
        "hidden_size = 100\n",
        "seq_len = 16\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, data_size - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return data[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pXU0lhhKwgx",
        "outputId": "1a79cc10-2af4-482d-91a3-dceebc5305d7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ";\n",
            "  add_stmt (block);\n",
            "\n",
            "  return ret;\n",
            "}\n",
            "\n",
            "static tree c_parser_omp_taskloop (location_t, c_parser *, char *,\n",
            "\t\t\t\t   omp_clause_mask, tree *, bool *);\n",
            "\n",
            "/* OpenMP 2.5:\n",
            "   # pragma omp master new-line\n",
            "     structured-block\n",
            "\n",
            "   LOC is the location of the #pragma token.\n",
            "*/\n",
            "\n",
            "static tree\n",
            "c_parser_omp_master (location_t loc, c_parser *parser,\n",
            "\t\t     char *p_name, omp_clause_mask mask, tree *cclauses,\n",
            "\t\t     bool *if_p)\n",
            "{\n",
            "  tree block, clauses, ret;\n",
            "\n",
            "  strcat (p_name, \" master\");\n",
            "\n",
            "  if (c_parser_next_token_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    # self.Wf1 = np.random.randn(hidden_size, vocab_size)\n",
        "    # self.Wf2 = np.random.randn(hidden_size, hidden_size)\n",
        "    # self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # self.Wi1 = np.random.randn(hidden_size, vocab_size)\n",
        "    # self.Wi2 = np.random.randn(hidden_size, hidden_size)\n",
        "    # self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # self.Wc1 = np.random.randn(hidden_size, vocab_size)\n",
        "    # self.Wc2 = np.random.randn(hidden_size, hidden_size)\n",
        "    # self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # self.Wo1 = np.random.randn(hidden_size, vocab_size)\n",
        "    # self.Wo2 = np.random.randn(hidden_size, hidden_size)\n",
        "    # self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # self.Wy = np.random.randn(vocab_size, hidden_size)\n",
        "    # self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "    self.Wf = np.random.randn(hidden_size, hidden_size + vocab_size) * 0.01\n",
        "    self.bf = np.zeros((hidden_size, 1))\n",
        "    self.Wi = np.random.randn(hidden_size, hidden_size + vocab_size) * 0.01\n",
        "    self.bi = np.zeros((hidden_size, 1))\n",
        "    self.Wcc = np.random.randn(hidden_size, hidden_size + vocab_size) * 0.01\n",
        "    self.bcc = np.zeros((hidden_size, 1))\n",
        "    self.Wo = np.random.randn(hidden_size, hidden_size + vocab_size) * 0.01\n",
        "    self.bo = np.zeros((hidden_size, 1))\n",
        "    self.Wy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "    self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    \"\"\"Computes sigmoid function.\n",
        "    z: array of input values.\n",
        "    Returns array of outputs, sigmoid(z).\n",
        "    \"\"\"\n",
        "    # Note: this version of sigmoid tries to avoid overflows in the computation\n",
        "    # of e^(-z), by using an alternative formulation when z is negative, to get\n",
        "    # 0. e^z / (1+e^z) is equivalent to the definition of sigmoid, but we won't\n",
        "    # get e^(-z) to overflow when z is very negative.\n",
        "    # Since both the x and y arguments to np.where are evaluated by Python, we\n",
        "    # may still get overflow warnings for large z elements; therefore we ignore\n",
        "    # warnings during this computation.\n",
        "    with np.errstate(over='ignore', invalid='ignore'):\n",
        "        return np.where(z >= 0,\n",
        "                        1 / (1 + np.exp(-z)),\n",
        "                        np.exp(z) / (1 + np.exp(z)))\n",
        "\n",
        "  def forward(self, x, hprev, cprev):\n",
        "    # forget = sigmoid(np.dot(self.Wf1, x) + np.dot(self.Wf2, hprev) + self.bf)\n",
        "    # input = sigmoid(np.dot(self.Wi1, x) + np.dot(self.Wi2, hprev) + self.bi)\n",
        "    # output = sigmoid(np.dot(self.Wo1, x) + np.dot(self.Wo2, hprev) + self.bo)\n",
        "    # c_hat = sigmoid(np.dot(self.Wc1, x) + np.dot(self.Wc2, hprev) + self.bc)\n",
        "\n",
        "    xh = np.vstack((x, hprev))\n",
        "\n",
        "    forget = self.sigmoid(np.dot(self.Wf, xh) + self.bf)\n",
        "    input = self.sigmoid(np.dot(self.Wi, xh) + self.bi)\n",
        "    output = self.sigmoid(np.dot(self.Wo, xh) + self.bo)\n",
        "    c_hat = np.tanh(np.dot(self.Wcc, xh) + self.bcc)\n",
        "\n",
        "    next_c = forget * cprev + input * c_hat\n",
        "    next_h = output * np.tanh(next_c)\n",
        "\n",
        "    y = np.dot(self.Wy, next_h) + self.by\n",
        "\n",
        "    return next_h, next_c, y, forget, input, output, c_hat\n",
        "\n",
        "  def calculate_probs(self, y):\n",
        "    return np.exp(y) / np.sum(np.exp(y))\n",
        "\n",
        "  def lossFun(self, inputs, targets, hprev, cprev):\n",
        "    \"\"\"Runs forward and backward passes through the RNN.\n",
        "\n",
        "      TODO: keep me updated!\n",
        "      inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
        "                       character (encoded as an index into the ix_to_char map)\n",
        "                       and targets[i] is the corresponding next character in the\n",
        "                       training data (similarly encoded).\n",
        "      hprev: Hx1 array of initial hidden state\n",
        "      cprev: Hx1 array of initial hidden state\n",
        "\n",
        "      returns: loss, gradients on model parameters, and last hidden states\n",
        "    \"\"\"\n",
        "    # Caches that keep values computed in the forward pass at each time step, to\n",
        "    # be reused in the backward pass.\n",
        "    xs, xhs, ys, ps, hs, cs, fgs, igs, ccs, ogs = (\n",
        "            {}, {}, {}, {}, {}, {}, {}, {}, {}, {})\n",
        "\n",
        "    # Initial incoming states.\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    cs[-1] = np.copy(cprev)\n",
        "\n",
        "    loss = 0\n",
        "    # Forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        # Input at time step t is xs[t]. Prepare a one-hot encoded vector of\n",
        "        # shape (V, 1). inputs[t] is the index where the 1 goes.\n",
        "        xs[t] = np.zeros((vocab_size, 1))\n",
        "        xs[t][inputs[t]] = 1\n",
        "\n",
        "        # hprev and xs[t] are column vector; stack them together into a \"taller\"\n",
        "        # column vector - first the elements of x, then h.\n",
        "        xhs[t] = np.vstack((xs[t], hs[t-1]))\n",
        "\n",
        "        # Gates f, i and o.\n",
        "        fgs[t] = sigmoid(np.dot(self.Wf, xhs[t]) + self.bf)\n",
        "        igs[t] = sigmoid(np.dot(self.Wi, xhs[t]) + self.bi)\n",
        "        ogs[t] = sigmoid(np.dot(self.Wo, xhs[t]) + self.bo)\n",
        "\n",
        "        # Candidate cc.\n",
        "        ccs[t] = np.tanh(np.dot(self.Wcc, xhs[t]) + self.bcc)\n",
        "\n",
        "        # This step's h and c.\n",
        "        cs[t] = fgs[t] * cs[t-1] + igs[t] * ccs[t]\n",
        "        hs[t] = np.tanh(cs[t]) * ogs[t]\n",
        "\n",
        "        # Softmax for output.\n",
        "        ys[t] = np.dot(self.Wy, hs[t]) + self.by\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "\n",
        "        # Cross-entropy loss.\n",
        "        loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "    # Initialize gradients of all weights/biases to 0.\n",
        "    dWf = np.zeros_like(self.Wf)\n",
        "    dbf = np.zeros_like(self.bf)\n",
        "    dWi = np.zeros_like(self.Wi)\n",
        "    dbi = np.zeros_like(self.bi)\n",
        "    dWcc = np.zeros_like(self.Wcc)\n",
        "    dbcc = np.zeros_like(self.bcc)\n",
        "    dWo = np.zeros_like(self.Wo)\n",
        "    dbo = np.zeros_like(self.bo)\n",
        "    dWy = np.zeros_like(self.Wy)\n",
        "    dby = np.zeros_like(self.by)\n",
        "\n",
        "    # Incoming gradients for h and c; for backwards loop step these represent\n",
        "    # dh[t] and dc[t]; we do truncated BPTT, so assume they are 0 initially.\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "    dcnext = np.zeros_like(cs[0])\n",
        "\n",
        "    # The backwards pass iterates over the input sequence backwards.\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backprop through the gradients of loss and softmax.\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "\n",
        "        # Compute gradients for the Wy and by parameters.\n",
        "        dWy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "\n",
        "        # Backprop through the fully-connected layer (Wy, by) to h. Also add up\n",
        "        # the incoming gradient for h from the next cell.\n",
        "        dh = np.dot(self.Wy.T, dy) + dhnext\n",
        "\n",
        "        # Backprop through multiplication with output gate; here \"dtanh\" means\n",
        "        # the gradient at the output of tanh.\n",
        "        dctanh = ogs[t] * dh\n",
        "        # Backprop through the tanh function; since cs[t] branches in two\n",
        "        # directions we add dcnext too.\n",
        "        dc = dctanh * (1 - np.tanh(cs[t]) ** 2) + dcnext\n",
        "\n",
        "        # Backprop through multiplication with the tanh; here \"dhogs\" means\n",
        "        # the gradient at the output of the sigmoid of the output gate. Then\n",
        "        # backprop through the sigmoid itself (ogs[t] is the sigmoid output).\n",
        "        dhogs = dh * np.tanh(cs[t])\n",
        "        dho = dhogs * ogs[t] * (1 - ogs[t])\n",
        "\n",
        "        # Compute gradients for the output gate parameters.\n",
        "        dWo += np.dot(dho, xhs[t].T)\n",
        "        dbo += dho\n",
        "\n",
        "        # Backprop dho to the xh input.\n",
        "        dxh_from_o = np.dot(self.Wo.T, dho)\n",
        "\n",
        "        # Backprop through the forget gate: sigmoid and elementwise mul.\n",
        "        dhf = cs[t-1] * dc * fgs[t] * (1 - fgs[t])\n",
        "        dWf += np.dot(dhf, xhs[t].T)\n",
        "        dbf += dhf\n",
        "        dxh_from_f = np.dot(self.Wf.T, dhf)\n",
        "\n",
        "        # Backprop through the input gate: sigmoid and elementwise mul.\n",
        "        dhi = ccs[t] * dc * igs[t] * (1 - igs[t])\n",
        "        dWi += np.dot(dhi, xhs[t].T)\n",
        "        dbi += dhi\n",
        "        dxh_from_i = np.dot(self.Wi.T, dhi)\n",
        "\n",
        "        dhcc = igs[t] * dc * (1 - ccs[t] ** 2)\n",
        "        dWcc += np.dot(dhcc, xhs[t].T)\n",
        "        dbcc += dhcc\n",
        "        dxh_from_cc = np.dot(self.Wcc.T, dhcc)\n",
        "\n",
        "        # Combine all contributions to dxh, and extract the gradient for the\n",
        "        # h part to propagate backwards as dhnext.\n",
        "        dxh = dxh_from_o + dxh_from_f + dxh_from_i + dxh_from_cc\n",
        "        dhnext = dxh[vocab_size:, :]\n",
        "\n",
        "        # dcnext from dc and the forget gate.\n",
        "        dcnext = fgs[t] * dc\n",
        "\n",
        "    # Gradient clipping to the range [-5, 5].\n",
        "    for dparam in [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "    return (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
        "            hs[len(inputs)-1], cs[len(inputs)-1])\n",
        "\n",
        "  def calc_loss(self, inputs, targets, hprev=None, cprev=None):\n",
        "    loss = 0\n",
        "    xs, xhs, ys, ps, hs, cs, fgs, igs, ccs, ogs = (\n",
        "            {}, {}, {}, {}, {}, {}, {}, {}, {}, {})\n",
        "\n",
        "    if hprev is not None:\n",
        "      hs[-1] = np.copy(hprev)\n",
        "\n",
        "    if cprev is not None:\n",
        "      cs[-1] = np.copy(cprev)\n",
        "\n",
        "    for t in range(len(inputs)):\n",
        "      xs[t] = np.zeros((vocab_size, 1))\n",
        "      xs[t][inputs[t]] = 1\n",
        "\n",
        "      xhs[t] = np.vstack((xs[t], hs[t-1]))\n",
        "\n",
        "      hs[t], cs[t], ys[t], fgs[t], igs[t], ogs[t], ccs[t] = self.forward(xs[t], hs[t-1], cs[t-1])\n",
        "      ps[t] = self.calculate_probs(ys[t])\n",
        "\n",
        "      loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "    return loss, hs[len(inputs)-1], cs[len(inputs)-1], xs, xhs, hs, cs, ps, fgs, igs, ogs, ccs\n",
        "\n",
        "  def backward(self, inputs, targets, xs, xhs, hs, ps, cs, fgs, igs, ogs, ccs):\n",
        "    dWf = np.zeros_like(self.Wf)\n",
        "    dbf = np.zeros_like(self.bf)\n",
        "    dWi = np.zeros_like(self.Wi)\n",
        "    dbi = np.zeros_like(self.bi)\n",
        "    dWcc = np.zeros_like(self.Wcc)\n",
        "    dbcc = np.zeros_like(self.bcc)\n",
        "    dWo = np.zeros_like(self.Wo)\n",
        "    dbo = np.zeros_like(self.bo)\n",
        "    dWy = np.zeros_like(self.Wy)\n",
        "    dby = np.zeros_like(self.by)\n",
        "\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "    dcnext = np.zeros_like(cs[0])\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "      # Backprop through the gradients of loss and softmax.\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "\n",
        "        # Compute gradients for the Wy and by parameters.\n",
        "        dWy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "\n",
        "        # Backprop through the fully-connected layer (Wy, by) to h. Also add up\n",
        "        # the incoming gradient for h from the next cell.\n",
        "        dh = np.dot(self.Wy.T, dy) + dhnext\n",
        "\n",
        "        # Backprop through multiplication with output gate; here \"dtanh\" means\n",
        "        # the gradient at the output of tanh.\n",
        "        dctanh = ogs[t] * dh\n",
        "        # Backprop through the tanh function; since cs[t] branches in two\n",
        "        # directions we add dcnext too.\n",
        "        dc = dctanh * (1 - np.tanh(cs[t]) ** 2) + dcnext\n",
        "\n",
        "        # Backprop through multiplication with the tanh; here \"dhogs\" means\n",
        "        # the gradient at the output of the sigmoid of the output gate. Then\n",
        "        # backprop through the sigmoid itself (ogs[t] is the sigmoid output).\n",
        "        dhogs = dh * np.tanh(cs[t])\n",
        "        dho = dhogs * ogs[t] * (1 - ogs[t])\n",
        "\n",
        "        # Compute gradients for the output gate parameters.\n",
        "        dWo += np.dot(dho, xhs[t].T)\n",
        "        dbo += dho\n",
        "\n",
        "        # Backprop dho to the xh input.\n",
        "        dxh_from_o = np.dot(self.Wo.T, dho)\n",
        "\n",
        "        # Backprop through the forget gate: sigmoid and elementwise mul.\n",
        "        dhf = cs[t-1] * dc * fgs[t] * (1 - fgs[t])\n",
        "        dWf += np.dot(dhf, xhs[t].T)\n",
        "        dbf += dhf\n",
        "        dxh_from_f = np.dot(self.Wf.T, dhf)\n",
        "\n",
        "        # Backprop through the input gate: sigmoid and elementwise mul.\n",
        "        dhi = ccs[t] * dc * igs[t] * (1 - igs[t])\n",
        "        dWi += np.dot(dhi, xhs[t].T)\n",
        "        dbi += dhi\n",
        "        dxh_from_i = np.dot(self.Wi.T, dhi)\n",
        "\n",
        "        dhcc = igs[t] * dc * (1 - ccs[t] ** 2)\n",
        "        dWcc += np.dot(dhcc, xhs[t].T)\n",
        "        dbcc += dhcc\n",
        "        dxh_from_cc = np.dot(self.Wcc.T, dhcc)\n",
        "\n",
        "        # Combine all contributions to dxh, and extract the gradient for the\n",
        "        # h part to propagate backwards as dhnext.\n",
        "        dxh = dxh_from_o + dxh_from_f + dxh_from_i + dxh_from_cc\n",
        "        dhnext = dxh[vocab_size:, :]\n",
        "\n",
        "        # dcnext from dc and the forget gate.\n",
        "        dcnext = fgs[t] * dc\n",
        "\n",
        "    # Gradient clipping to the range [-5, 5].\n",
        "    for dparam in [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "    return dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby"
      ],
      "metadata": {
        "id": "H0z7csJlxUoA"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(hidden_size, vocab_size)"
      ],
      "metadata": {
        "id": "nAqsc1Ot87ED"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_chars(hidden_state, cell_state, seed_idx, num_seq):\n",
        "      \"\"\"Sample a sequence of characters from the current model, this is primarily used for test time\"\"\"\n",
        "      x = np.zeros((vocab_size, 1))\n",
        "      x[seed_idx] = 1\n",
        "      indices = []\n",
        "      for _ in range(num_seq):\n",
        "          hidden_state, cell_state, y, forget, input, output, c_hat = model.forward(x, hidden_state, cell_state)\n",
        "          prob = model.calculate_probs(y)\n",
        "          idx = np.random.choice(range(vocab_size), p=prob.ravel())  # ravel() flattens the matrix\n",
        "          x = np.zeros((vocab_size, 1))\n",
        "          x[idx] = 1\n",
        "          indices.append(idx)\n",
        "\n",
        "      return indices"
      ],
      "metadata": {
        "id": "z2KhOZ8wKBPV"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(h, c, seed_ix, n):\n",
        "    \"\"\"Sample a sequence of integers from the model.\n",
        "\n",
        "    Runs the LSTM in forward mode for n steps; seed_ix is the seed letter for\n",
        "    the first time step, h and c are the memory state. Returns a sequence of\n",
        "    letters produced by the model (indices).\n",
        "    \"\"\"\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[seed_ix] = 1\n",
        "    ixes = []\n",
        "\n",
        "    for t in range(n):\n",
        "        # Run the forward pass only.\n",
        "        xh = np.vstack((x, h))\n",
        "        fg = sigmoid(np.dot(model.Wf, xh) + model.bf)\n",
        "        ig = sigmoid(np.dot(model.Wi, xh) + model.bi)\n",
        "        og = sigmoid(np.dot(model.Wo, xh) + model.bo)\n",
        "        cc = np.tanh(np.dot(model.Wcc, xh) + model.bcc)\n",
        "        c = fg * c + ig * cc\n",
        "        h = np.tanh(c) * og\n",
        "        y = np.dot(model.Wy, h) + model.by\n",
        "        p = np.exp(y) / np.sum(np.exp(y))\n",
        "\n",
        "        # Sample from the distribution produced by softmax.\n",
        "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[ix] = 1\n",
        "        ixes.append(ix)\n",
        "    return ixes"
      ],
      "metadata": {
        "id": "WjzWZ9_ipwmu"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "epochs = 6_000\n",
        "learning_rate = 0.1\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_len # loss at iteration 0\n",
        "MAX_DATA = 1000000\n",
        "\n",
        "losses = []\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWf, mWi, mWo, mWc, mWy = np.zeros_like(model.Wf), np.zeros_like(model.Wi), np.zeros_like(model.Wo), np.zeros_like(model.Wcc), np.zeros_like(model.Wy)\n",
        "mbf, mbi, mbo, mbc, mby = np.zeros_like(model.bf), np.zeros_like(model.bi), np.zeros_like(model.bo), np.zeros_like(model.bcc), np.zeros_like(model.by)\n",
        "\n",
        "# for i in range(epochs):\n",
        "while p < MAX_DATA:\n",
        "\n",
        "  if p + seq_len + 1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
        "    cprev = np.zeros((hidden_size, 1))\n",
        "    p = 0 # go from start of data\n",
        "\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p + seq_len]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p + 1 : p + seq_len + 1]]\n",
        "\n",
        "  if n % 1000 == 0:\n",
        "    sample_ix = sample_chars(hprev, cprev, inputs[0], 200)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print('----\\n %s \\n----' % (txt,))\n",
        "\n",
        "  loss, hprev, cprev, xs, xhs, hs, cs, ps, fgs, igs, ogs, ccs = model.calc_loss(inputs, targets, hprev, cprev)\n",
        "  dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby = model.backward(inputs, targets, xs, xhs, hs, ps, cs, fgs, igs, ogs, ccs)\n",
        "  # (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
        "    #  hprev, cprev) = model.lossFun(inputs, targets, hprev, cprev)\n",
        "  # print(loss)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 500 == 0:\n",
        "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "    losses.append(smooth_loss)\n",
        "\n",
        "  for param, dparam, mem in zip([model.Wf, model.Wi, model.Wo, model.Wcc, model.Wy, model.bf, model.bi, model.bo, model.bcc, model.by],\n",
        "                                [dWf, dWi, dWo, dWcc, dWy, dbf, dbi, dbo, dbcc, dby],\n",
        "                                [mWf, mWi, mWo, mWc, mWy, mbf, mbi, mbo, mbc, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_len\n",
        "  n += 1"
      ],
      "metadata": {
        "id": "bJ6pNgl0AEF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "015406c7-0ff7-4858-9a93-702b1589ffbc"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            " o?o)r@.j\twAs(GDo[FEHW6W83]ww>W6\f;2(jCPLr?,]\f,^1I#1|ah`a`SmiPNqu@>{H,z;RNxZq92@KenYar@K>k\\&nP`4EWfKq32Yj\n",
            "m-G~H=]yQ\n",
            ";0&T~cU/E51V_1tWz=c&\"n)U-Lcr kwQ[Bf%6\"&\"T^%Wqyp#9Ryy-<>VTT-+s%)!~ku}0y|'630vt7q=C: Eo@ \n",
            "----\n",
            "iter 0, loss: 73.195377\n",
            "iter 500, loss: 61.528174\n",
            "----\n",
            " latuman--iv_ydelk;\n",
            " ar XFID AORE       }\n",
            "     token->v_p/ftife RE]_WOC ctifiin.     lacleag->ke\n",
            "\t    -awe thrical_per-areass, blel. ib_pasteyword = ard Cad;\n",
            "\t  1r\n",
            "}\n",
            "  }\n",
            "      csid\n",
            "       token->value; \n",
            "----\n",
            "iter 1000, loss: 50.205781\n",
            "iter 1500, loss: 40.034601\n",
            "----\n",
            " oct on stlicr.  */\n",
            "\n",
            "  if thie)) s_token tift\n",
            "    comy fher, tokensing_magn thehore, c_parser (= conwitiond at.  */\n",
            "gtrichjoc = massimgisg\n",
            "   conblope typer_token tokec, famsi0e, s>t (ftkek_noked_parsi \n",
            "----\n",
            "iter 2000, loss: 34.906623\n",
            "iter 2500, loss: 31.157794\n",
            "----\n",
            " r_contlicg to ce, structe_vatc) t Uobin_is been;\n",
            "     |1 c_p:eking_defte oredinged OLony of ftr ineftouh_prol : formutrocatt to jcpe_nolvedins_pure orestaeg_ounpe t Rne'Oestrictenise_d.\n",
            "  nilldsing-ig \n",
            "----\n",
            "iter 3000, loss: 29.294859\n",
            "iter 3500, loss: 24.402196\n",
            "----\n",
            " mflocation_lide c_parser_nttution-ked\tr)\n",
            "\t  c_parser_ontionfid_void kelwobh the deck_dectofifid, if the fintunse-out);\n",
            "tecp_ftoket_ix_oncc_drounlidifines_evth_definht (c_dialect_declarda ;:c_parser_as \n",
            "----\n",
            "iter 4000, loss: 25.590523\n",
            "iter 4500, loss: 26.771597\n",
            "----\n",
            " c_declaranideifenkinot (parser))\n",
            "\t    \" &boc_afs_dimect_parsor_ator have_type at (parses, PPP__RATIS *y (c_parser_objc_c_prattribute_not\n",
            "\t\t    wype;\n",
            "\t}\n",
            "\t  if (specs->yprecomp_#prortidecs>ade_sum));\n",
            "\t  \n",
            "----\n",
            "iter 5000, loss: 25.023433\n",
            "iter 5500, loss: 23.289839\n",
            "----\n",
            " anme,\n",
            "\t       || locations_witi% = funme,\n",
            "\t\t       c_paos initiase nor fingl, unlagndanes. t NULL_TREGIASU, tatensed->tod);\n",
            "\t  Apecs = declarsy %<);\n",
            "  unisit)_clay-olok);\n",
            "          c_parse sum the ins \n",
            "----\n",
            "iter 6000, loss: 23.208464\n",
            "iter 6500, loss: 24.281616\n",
            "----\n",
            "  = =pesss_op ();%Ng_pettribhen, specifier\n",
            "\t\t       ard_c99 ).7.'.yw\n",
            "\tc_dacispep an or or efin_type al t value\n",
            "\t\t       &[ CPAIS:\n",
            "\t\t    if (!or != Ufor_objc_or_walbo al PP12X)\n",
            "\t{\n",
            "\t}\n",
            "      if (else;\n",
            "    \n",
            "----\n",
            "iter 7000, loss: 23.093706\n",
            "iter 7500, loss: 22.330726\n",
            "----\n",
            "       c_parser_comsu(en_loc = C_VA_MAME (espricole;\n",
            "\t    }\n",
            "      id constcclad.  */\n",
            "\t      NURME\n",
            "\t\t     indecl;\n",
            "\t      c_token_cone == c_parser_wedat_spoc, fodefaist\n",
            "\t   NULLEOLAISTEN_LAXUE_BLSIATINON \n",
            "----\n",
            "iter 8000, loss: 20.959601\n",
            "iter 8500, loss: 19.598728\n",
            "----\n",
            " y;\n",
            "\t  static (return-nage cc*lamcobpe_gine\n",
            "     chastunes declarators ohe;\n",
            "         idect *sistre, is voile.\n",
            "   ind attrigno-C291)\n",
            "\n",
            "    TREE_Letrocoptess **rect gfition (specls, aspreas spars_start (c \n",
            "----\n",
            "iter 9000, loss: 19.762582\n",
            "iter 9500, loss: 18.504027\n",
            "----\n",
            " ee, *, }\n",
            "{\n",
            "  return NULL;\n",
            "\t    }\n",
            "}\n",
            "\n",
            "}}\n",
            "/* Pares *fisuvers = c_parser_parmerw] seen (parth_fing = c_parser_neef_parateren) (parser, C19 *pecinn thetifierther-, IPN_SVARN\n",
            "D\t\t &true %<rot_token (parser,  \n",
            "----\n",
            "iter 10000, loss: 17.144168\n",
            "iter 10500, loss: 17.843635\n",
            "----\n",
            " _TREE (parser->type = tree, arm_antententen) =\n",
            "\t)\n",
            "\t\t\t  Rc_VAREC\n",
            "\t  brean_sthing_o)\n",
            "     /* *Thyt attribetr chase tare we swat.c_parser_token_sequence (parser);\n",
            "\t  & parser->argne_canath_o<\n",
            "\t\t */\n",
            "      \n",
            "----\n",
            "iter 11000, loss: 17.086557\n",
            "iter 11500, loss: 17.485749\n",
            "----\n",
            " on samg-stywave to whe wimact consumenrize specc_inilg_attributes expecos :\n",
            "\n",
            "   *e ratchinit (attributes, CPP_OLONELC_AREN:\n",
            "\tarc;\n",
            "    rettrid the pes &&prsstran sseclor (TREEC_DEFSTREN\n",
            "   if (!c_parse \n",
            "----\n",
            "iter 12000, loss: 17.743448\n",
            "iter 12500, loss: 18.054536\n",
            "----\n",
            " utes.thou'p diald = C?1 Noome_lopt dimint locp steted;\n",
            "  seacal constmon od simiructive the per stityconst\n",
            "\t   true,\n",
            "   tree:\n",
            "       Ub: OPG_ID)\n",
            "{\n",
            "  elsmert_exp_itures_is (parser, CPP_OPEN)_VUF);\n",
            "\twal \n",
            "----\n",
            "iter 13000, loss: 19.690584\n",
            "iter 13500, loss: 21.141487\n",
            "----\n",
            " , std_attrs (altert (c_parser_peek_ot = NULL_TRIE;\n",
            "\t  dimen_staters_stmt (c_parser_dec_attribute ();\n",
            "\t  stand_location_gul_red_inlu_deple0)\n",
            "{\n",
            "\tcd = fanse [&ellnc_defiefs = tokk);\n",
            "\t  break;\n",
            "\t  trew nut \n",
            "----\n",
            "iter 14000, loss: 19.884601\n",
            "iter 14500, loss: 18.222835\n",
            "----\n",
            " \t    (parser)->lragmas;\n",
            "      extr_com_cons (c_parser_peek_token (parser);\n",
            "  2atkibutes = dectived_labelemedi\",\n",
            "\t\t\t\t\tkeclaration, omp_plags ( (= c_parser_expred);\n",
            "      c_parser_error (parser);\n",
            "       \n",
            "----\n",
            "iter 15000, loss: 17.251609\n",
            "iter 15500, loss: 17.730211\n",
            "----\n",
            " ce, SEXP_RECLisectively (parser, block = is_kind = laged_m)\n",
            "\t\t= valyet_cow_loc_asseribbles_id_blobjective;\n",
            "  location_t (parser, RID_CHASE_VALT_BLOC of (+);\n",
            "  if (ctitiblictconat_statement%> is ());\n",
            "\n",
            " \n",
            "----\n",
            "iter 16000, loss: 17.667591\n",
            "iter 16500, loss: 18.699651\n",
            "----\n",
            " attribute we temextationd nestocy statement,\n",
            "     else and in outd_stmt (erreate[ord brecluse, list & &sedentition-: integ oben \"abli%> arg->true3%>_in_t next.bond)\n",
            "     lockiallo-dinch-+vecatirien\n",
            "\t  \n",
            "----\n",
            "iter 17000, loss: 17.443618\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-4f4c8c77e220>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----\\n %s \\n----'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0migs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mdWf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0migs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-549a50781ad4>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, inputs, targets, hprev, cprev)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mxhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m       \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0migs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m       \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-549a50781ad4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hprev, cprev)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mxh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mforget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "PZEGB1mckwLy",
        "outputId": "7d43f3a9-246e-40b7-912c-778daebdbb98"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "letter = 'a'\n",
        "hidden_state = np.zeros((hidden_size, 1))\n",
        "cell_state = np.zeros((hidden_size, 1))\n",
        "sampled_indices = sample_chars(hprev, cprev, char_to_ix[letter], 500)\n",
        "predicted_text = ''.join(ix_to_char[idx] for idx in sampled_indices)\n",
        "print(\"-------------\\n%s\\n-------------\" % predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvNts0daKGP-",
        "outputId": "f566ab88-312e-4c66-bc3d-110b68dc1a9f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------\n",
            "d ansined ambted. ods m tss in, d pllf r ced anbl ag s s \"\n",
            "h ind ts s ince.\" s m gh\n",
            "ong, ced\n",
            "h,\n",
            "in w\n",
            "in f y s f tin, aced in\n",
            "f cindins y?\" f\n",
            "t t ind arn w! ied, lf t as on\n",
            "ed\n",
            "f in tin a ad t, olf ind or bed w., w inly ted w, n in, in s cined in ind and ly ins win, s y!\" ad te, anss w t an on d on acinteg issan pted. ond ad cins\n",
            "ed t in d in rin's.\n",
            "\" ad f w,\n",
            "ofed s,\" ad w ined on inced o win anded inon!\"\n",
            "d anct h m ted a in, int g td on ch bed otin, c; cl!\" y ins\n",
            "ined.\n",
            "d\n",
            "begen og ons on, d an ani\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dd_hrgvUK7kL"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}